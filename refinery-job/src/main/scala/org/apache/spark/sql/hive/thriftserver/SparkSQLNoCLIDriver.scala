/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql.hive.thriftserver

import org.apache.commons.lang.StringUtils
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.hive.cli.{CliDriver, CliSessionState}
import org.apache.hadoop.hive.conf.HiveConf
import org.apache.hadoop.hive.ql.Driver
import org.apache.hadoop.hive.ql.processors._
import org.apache.hadoop.hive.ql.session.SessionState
import org.apache.hadoop.security.{Credentials, UserGroupInformation}
import org.apache.spark.SparkConf
import org.apache.spark.deploy.SparkHadoopUtil
import org.apache.spark.sql.hive.security.HiveDelegationTokenProvider
import org.apache.spark.internal.Logging
import org.apache.spark.sql.hive.HiveUtils
import org.apache.spark.sql.hive.client.HiveClientImpl
import org.apache.spark.sql.internal.SharedState
import org.apache.spark.sql.{AnalysisException, DataFrame}
import org.wikimedia.analytics.refinery.hive.HiveCLIOptionsProcessor

import java.io._
import java.util.{Locale, ArrayList => JArrayList, List => JList}
import scala.collection.JavaConverters._

/**
  * This code is an updated copy of {{@link org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver}}.
  * The package it is stored in is the original spark one, so that it can access protected and
  * private-to-package spark classes.
  *
  * It allows us to use to run SQL queries in cluster mode by removing the CLI aspect of the
  * driver while keeping the query-from-command and query-from-file running modes.
  *
  * Example usage:
  *
  * spark2-submit --master yarn --deploy-mode cluster \
  * --class org.apache.spark.sql.hive.thriftserver.SparkSQLNoCLIDriver \
  * /srv/deployment/analytics/refinery/artifacts/refinery-spark.jar \
  * -f hdfs://path/to/an/hql/query/file.hql
  * -d var1=value1
  * -d var2=value2
  */
object SparkSQLNoCLIDriver extends Logging {
    private final val SPARK_HADOOP_PROP_PREFIX = "spark.hadoop."

    /**
      * Launches the SQLNoCLIDriver with default CLI OptionProcessor parsing
      * and default SparkConf.
      * @param args The CLI arguments passed to the job
      */
    def main(args: Array[String]): Unit = {
        val optionsProcessor = new HiveCLIOptionsProcessor
        if (optionsProcessor.parseArgs(args)) {
            apply(optionsProcessor)
        }
    }

    /**
      * Wrapper over the driver execution allowing to reuse the HQL execution for other jobs
      * as it returns the DataFrame generated by the latest executed query
      *
      * @param args The CLI arguments passed to the jobs
      * @param oproc The CLI OptionProcessor to be used to parse the CLI args
      *              (should extend [[HiveCLIOptionsProcessor]]
      * @param sparkConf The SparkConf used to initialize the spark execution context
      * @return The dataframe resulting from the latest executed query
      */
    def apply(oproc: HiveCLIOptionsProcessor): DataFrame = {

        oproc.process_stage1()

        val sparkConf = new SparkConf(loadDefaults = true)
        val hadoopConf = SparkHadoopUtil.get.newConfiguration(sparkConf)
        val extraConfigs = HiveUtils.formatTimeVarsForHiveClient(hadoopConf)

        val cliConf = HiveClientImpl.newHiveConf(sparkConf, hadoopConf, extraConfigs)

        val sessionState = new CliSessionState(cliConf)

        if (!oproc.process_stage2(sessionState)) {
            logError("Problem processing command parameters.")
            throw new IllegalArgumentException("Problem processing command parameters.")
        }

        // Set all properties specified via command line.
        val conf = sessionState.getConf

        // Hive 2.0.0 onwards HiveConf.getClassLoader returns the UDFClassLoader (created by Hive).
        // Because of this spark cannot find the jars as class loader got changed
        // Hive changed the class loader because of HIVE-11878, so it is required to use old
        // classLoader as sparks loaded all the jars in this classLoader
        conf.setClassLoader(Thread.currentThread().getContextClassLoader)

        sessionState.cmdProperties.entrySet().asScala.foreach { item =>
            val key = item.getKey.toString
            val value = item.getValue.toString
            // We do not propagate metastore options to the execution copy of hive.
            if (key != "javax.jdo.option.ConnectionURL") {
                conf.set(key, value)
                sessionState.getOverriddenConfigurations.put(key, value)
            }
        }

        val tokenProvider = new HiveDelegationTokenProvider()
        if (tokenProvider.delegationTokensRequired(sparkConf, hadoopConf)) {
            val credentials = new Credentials()
            tokenProvider.obtainDelegationTokens(hadoopConf, sparkConf, credentials)
            UserGroupInformation.getCurrentUser.addCredentials(credentials)
        }

        SharedState.resolveWarehousePath(sparkConf, conf)
        SessionState.start(sessionState)

        // Respect the configurations set by --hiveconf from the command line
        // (based on Hive's CliDriver).
        val hiveConfFromCmd = sessionState.getOverriddenConfigurations.entrySet().asScala
        val newHiveConf = hiveConfFromCmd.map { kv =>
            // If the same property is configured by spark.hadoop.xxx, we ignore it and
            // obey settings from spark properties
            val k = kv.getKey
            val v = sys.props.getOrElseUpdate(SPARK_HADOOP_PROP_PREFIX + k, kv.getValue)
            (k, v)
        }

        val noCli = new SparkSQLNoCLIDriver
        noCli.setHiveVariables(oproc.getHiveVariables)

        // In SparkSQL CLI, we may want to use jars augmented by hiveconf
        // hive.aux.jars.path, here we add jars augmented by hiveconf to
        // Spark's SessionResourceLoader to obtain these jars.
        val auxJars = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEAUXJARS)
        if (StringUtils.isNotBlank(auxJars)) {
            val resourceLoader = WMFSparkSQLEnv.sqlContext.sessionState.resourceLoader
            StringUtils.split(auxJars, ",").foreach(resourceLoader.addJar(_))
        }

        // The class loader of CliSessionState's conf is current main thread's class loader
        // used to load jars passed by --jars. One class loader used by AddJarCommand is
        // sharedState.jarClassLoader which contain jar path passed by --jars in main thread.
        // We set CliSessionState's conf class loader to sharedState.jarClassLoader.
        // Thus we can load all jars passed by --jars and AddJarCommand.
        sessionState.getConf.setClassLoader(WMFSparkSQLEnv.sqlContext.sharedState.jarClassLoader)

        if (sessionState.database != null) {
            WMFSparkSQLEnv.sqlContext.sessionState.catalog.setCurrentDatabase(
                s"${sessionState.database}")
        }

        // Execute -i init files (always in silent mode)
        noCli.processInitFiles(sessionState)

        // We don't propagate hive.metastore.warehouse.dir, because it might has been adjusted in
        // [[SharedState.loadHiveConfFile]] based on the user specified or default values of
        // spark.sql.warehouse.dir and hive.metastore.warehouse.dir.
        for ((k, v) <- newHiveConf if k != "hive.metastore.warehouse.dir") {
            WMFSparkSQLEnv.sqlContext.setConf(k, v)
        }

        if (sessionState.execString != null) {
            val returnCode = noCli.processLine(sessionState.execString)
            if (returnCode != 0) {
                logError("Problem processing the HQL query from command line")
                throw new IllegalStateException("Problem processing HQL")
            }
            logInfo("Successfully processed SQL query from command line")
        } else {
            try {
                if (sessionState.fileName != null) {
                    val returnCode = noCli.processFile(sessionState.fileName)
                    if (returnCode != 0) {
                        logError("Problem processing the HQL query from file")
                        throw new IllegalStateException("Problem processing HQL")
                    }
                    logInfo("Successfully processed SQL query from file")
                }
            } catch {
                case e: FileNotFoundException =>
                    logError(s"Could not open input file for reading. (${e.getMessage})")
                    throw new IllegalStateException("Problem processing HQL", e)
            }
        }
        // Not closing the Hive-SQL session
        // This closes the classloader and prevents reusing it !
        //sessionState.close()

        // Return the driver latest computed result
        noCli.getLatestResult
    }

    def isRemoteMode(state: CliSessionState): Boolean = {
        //    sessionState.isRemoteMode
        state.isHiveServerQuery
    }

}

class SparkSQLNoCLIDriver extends CliDriver with Logging {
    private var latestResult: DataFrame = _

    private val sessionState = SessionState.get().asInstanceOf[CliSessionState]

    private val isRemoteMode = {
        SparkSQLNoCLIDriver.isRemoteMode(sessionState)
    }

    private val conf: Configuration =
        if (sessionState != null) sessionState.getConf else new Configuration()

    // Force initializing WMFSparkSQLEnv. This is put here but not object SparkSQLCliDriver
    // because the Hive unit tests do not go through the main() code path.
    if (!isRemoteMode) {
        WMFSparkSQLEnv.init()
    } else {
        // Hive 1.2 + not supported in CLI
        throw new RuntimeException("Remote operations not supported")
    }

    override def setHiveVariables(hiveVariables: java.util.Map[String, String]): Unit = {
        hiveVariables.asScala.foreach(kv => WMFSparkSQLEnv.sqlContext.conf.setConfString(kv._1, kv._2))
    }

    override def processCmd(cmd: String): Int = {
        val cmd_trimmed: String = cmd.trim()
        val cmd_lower = cmd_trimmed.toLowerCase(Locale.ROOT)
        val tokens: Array[String] = cmd_trimmed.split("\\s+")
        val cmd_1: String = cmd_trimmed.substring(tokens(0).length()).trim()

        if (cmd_lower.equals("quit") ||
            cmd_lower.equals("exit")) {
            logError(s"Can't run command '$cmd_trimmed' in No-CLI mode")
            1
        }
        if (tokens(0).toLowerCase(Locale.ROOT).equals("source") ||
            cmd_trimmed.startsWith("!") || isRemoteMode) {
            logError(s"Can't run command '$cmd_trimmed' in No-CLI mode")
            1
        } else {
            var ret = 0
            val hconf = conf.asInstanceOf[HiveConf]
            val proc: CommandProcessor = CommandProcessorFactory.get(tokens, hconf)
            if (proc != null) {
                if (proc.isInstanceOf[Driver] || proc.isInstanceOf[SetProcessor] ||
                    proc.isInstanceOf[AddResourceProcessor] || proc.isInstanceOf[ListResourceProcessor] ||
                    proc.isInstanceOf[ResetProcessor]) {
                    val driver = new WMFSparkSQLDriver

                    driver.init()
                    val start: Long = System.currentTimeMillis()
                    val rc = driver.run(cmd)
                    val end = System.currentTimeMillis()
                    val timeTaken: Double = (end - start) / 1000.0

                    ret = rc.getResponseCode
                    if (ret != 0) {
                        // For analysis exception, only the error is printed out to the console.
                        rc.getException match {
                            case e: AnalysisException =>
                                logError(s"""Error in query: ${e.getMessage}""")
                            case _ => logError(rc.getErrorMessage)
                        }
                        driver.close()
                        return ret
                    }

                    logInfo(s"Took ${timeTaken}s to execute query ${cmd}")
                    if (driver.getDataFrame != null) {
                        latestResult = driver.getDataFrame
                    }

                    val cret = driver.close()
                    if (ret == 0) {
                        ret = cret
                    }

                    // Destroy the driver to release all the locks.
                    driver.destroy()
                } else {
                    if (sessionState.getIsVerbose) {
                        logError(s"Can't run command '${tokens(0)} $cmd_1' in No-CLI mode")
                    }
                    ret = 1
                }
            }
            ret
        }
    }

    // Adapted processLine from Hive 2.3's CliDriver.processLine.
    override def processLine(line: String, allowInterrupting: Boolean): Int = {
        var lastRet: Int = 0

        // we can not use "split" function directly as ";" may be quoted
        val commands = splitSemiColon(line).asScala
        var command: String = ""
        for (oneCmd <- commands) {
            if (StringUtils.endsWith(oneCmd, "\\")) {
                command += StringUtils.chop(oneCmd) + ";"
            } else {
                command += oneCmd
                if (!StringUtils.isBlank(command)) {
                    val ret = processCmd(command)
                    command = ""
                    lastRet = ret
                    val ignoreErrors = HiveConf.getBoolVar(conf, HiveConf.ConfVars.CLIIGNOREERRORS)
                    if (ret != 0 && !ignoreErrors) {
                        CommandProcessorFactory.clean(conf.asInstanceOf[HiveConf])
                        return ret
                    }
                }
            }
        }
        CommandProcessorFactory.clean(conf.asInstanceOf[HiveConf])
        lastRet
    }

    // Adapted splitSemiColon from Hive 2.3's CliDriver.splitSemiColon.
    // Note: [SPARK-31595] if there is a `'` in a double quoted string, or a `"` in a single quoted
    // string, the origin implementation from Hive will not drop the trailing semicolon as expected,
    // hence we refined this function a little bit.
    // Note: [SPARK-33100] Ignore a semicolon inside a bracketed comment in spark-sql.
    private def splitSemiColon(line: String): JList[String] = {
        var insideSingleQuote = false
        var insideDoubleQuote = false
        var insideSimpleComment = false
        var bracketedCommentLevel = 0
        var escape = false
        var beginIndex = 0
        var leavingBracketedComment = false
        var isStatement = false
        val ret = new JArrayList[String]

        def insideBracketedComment: Boolean = bracketedCommentLevel > 0
        def insideComment: Boolean = insideSimpleComment || insideBracketedComment
        def statementInProgress(index: Int): Boolean = isStatement || (!insideComment &&
            index > beginIndex && !s"${line.charAt(index)}".trim.isEmpty)

        for (index <- 0 until line.length) {
            // Checks if we need to decrement a bracketed comment level; the last character '/' of
            // bracketed comments is still inside the comment, so `insideBracketedComment` must keep true
            // in the previous loop and we decrement the level here if needed.
            if (leavingBracketedComment) {
                bracketedCommentLevel -= 1
                leavingBracketedComment = false
            }

            if (line.charAt(index) == '\'' && !insideComment) {
                // take a look to see if it is escaped
                // See the comment above about SPARK-31595
                if (!escape && !insideDoubleQuote) {
                    // flip the boolean variable
                    insideSingleQuote = !insideSingleQuote
                }
            } else if (line.charAt(index) == '\"' && !insideComment) {
                // take a look to see if it is escaped
                // See the comment above about SPARK-31595
                if (!escape && !insideSingleQuote) {
                    // flip the boolean variable
                    insideDoubleQuote = !insideDoubleQuote
                }
            } else if (line.charAt(index) == '-') {
                val hasNext = index + 1 < line.length
                if (insideDoubleQuote || insideSingleQuote || insideComment) {
                    // Ignores '-' in any case of quotes or comment.
                    // Avoids to start a comment(--) within a quoted segment or already in a comment.
                    // Sample query: select "quoted value --"
                    //                                    ^^ avoids starting a comment if it's inside quotes.
                } else if (hasNext && line.charAt(index + 1) == '-') {
                    // ignore quotes and ; in simple comment
                    insideSimpleComment = true
                }
            } else if (line.charAt(index) == ';') {
                if (insideSingleQuote || insideDoubleQuote || insideComment) {
                    // do not split
                } else {
                    if (isStatement) {
                        // split, do not include ; itself
                        ret.add(line.substring(beginIndex, index))
                    }
                    beginIndex = index + 1
                    isStatement = false
                }
            } else if (line.charAt(index) == '\n') {
                // with a new line the inline simple comment should end.
                if (!escape) {
                    insideSimpleComment = false
                }
            } else if (line.charAt(index) == '/' && !insideSimpleComment) {
                val hasNext = index + 1 < line.length
                if (insideSingleQuote || insideDoubleQuote) {
                    // Ignores '/' in any case of quotes
                } else if (insideBracketedComment && line.charAt(index - 1) == '*' ) {
                    // Decrements `bracketedCommentLevel` at the beginning of the next loop
                    leavingBracketedComment = true
                } else if (hasNext && !insideBracketedComment && line.charAt(index + 1) == '*') {
                    bracketedCommentLevel += 1
                }
            }
            // set the escape
            if (escape) {
                escape = false
            } else if (line.charAt(index) == '\\') {
                escape = true
            }

            isStatement = statementInProgress(index)
        }
        if (isStatement) {
            ret.add(line.substring(beginIndex))
        }
        ret
    }

    def getLatestResult: DataFrame = latestResult
}
